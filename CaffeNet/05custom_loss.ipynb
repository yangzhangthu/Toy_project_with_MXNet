{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "def get_iterators(batch_size, data_shape=(3, 224, 224)):\n",
    "    train = mx.io.ImageRecordIter(\n",
    "#         path_imgrec         = '/data4/srip_face/img/jump_detector/old/jump_train_old.rec', \n",
    "        path_imgrec         = '/data4/srip_face/img/jump_detector/jump_train.rec', \n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = data_shape,\n",
    "        shuffle             = True,\n",
    "        rand_crop           = True,\n",
    "        rand_mirror         = True)\n",
    "    val = mx.io.ImageRecordIter(\n",
    "#         path_imgrec         = '/data4/srip_face/img/jump_detector/old/jump_valid_old.rec',\n",
    "        path_imgrec         = '/data4/srip_face/img/jump_detector/jump_valid.rec',\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = data_shape,\n",
    "        rand_crop           = False,\n",
    "        rand_mirror         = False)\n",
    "    return (train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class WeightedLogisticRegression(mx.operator.CustomOp):\n",
    "    def __init__(self, neg_w, pos_w):\n",
    "        self.neg_w = float(neg_w)\n",
    "        self.pos_w = float(pos_w)\n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        x = in_data[0].asnumpy()\n",
    "        y = np.exp(x - x.max(axis=1).reshape((x.shape[0], 1)))\n",
    "        y /= y.sum(axis=1).reshape((x.shape[0], 1))\n",
    "        self.assign(out_data[0], req[0], mx.nd.array(y))\n",
    "    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        l = in_data[1].asnumpy().ravel().astype(np.int)\n",
    "        y = out_data[0].asnumpy()\n",
    "        \n",
    "        w0 = self.neg_w\n",
    "        w1 = self.pos_w\n",
    "        \n",
    "        # fg wrong\n",
    "        mask0 = np.ones(y.shape, dtype=float)\n",
    "        mask0[:,0] = 0.0\n",
    "        mask0[:,l] = 0.0\n",
    "        \n",
    "        # bg wrong\n",
    "        mask1 = np.ones(y.shape, dtype=float)\n",
    "        mask1[:,1] = 0.0\n",
    "        mask1[:,l] = 0.0\n",
    "        \n",
    "        # fg right\n",
    "        mask2 = np.zeros(y.shape, dtype=float)\n",
    "        mask2[:,l] = 1.0\n",
    "        mask2[:,0] = 0.0\n",
    "        \n",
    "        # bg right\n",
    "        mask3 = np.zeros(y.shape, dtype=float)\n",
    "        mask3[:,l] = 1.0\n",
    "        mask3[:,1] = 0.0\n",
    "        \n",
    "        mask = mask0*w0 + mask1*w1 + mask2*w1 + mask3*w0\n",
    "        \n",
    "        y[np.arange(l.shape[0]), l] -= 1.0\n",
    "        y *= mask\n",
    "        self.assign(in_grad[0], req[0], mx.nd.array(y))\n",
    "\n",
    "@mx.operator.register(\"weighted_logistic_regression\")\n",
    "class WeightedLogisticRegressionProp(mx.operator.CustomOpProp):\n",
    "    def __init__(self, neg_w, pos_w):\n",
    "        self.neg_w = float(neg_w)\n",
    "        self.pos_w = float(pos_w)\n",
    "        super(WeightedLogisticRegressionProp, self).__init__(False)\n",
    "    def list_arguments(self):\n",
    "        return ['data', 'label']\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "    def infer_type(self, in_type):\n",
    "        dtype = in_type[0]\n",
    "        return [dtype, dtype], [dtype], []\n",
    "    def infer_shape(self, in_shape):\n",
    "        data_shape = in_shape[0]\n",
    "        label_shape = (in_shape[0][0],)\n",
    "        output_shape = in_shape[0]\n",
    "        return [data_shape, label_shape], [output_shape], []\n",
    "    def create_operator(self, ctx, shapes, dtypes):\n",
    "        return WeightedLogisticRegression(self.neg_w, self.pos_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym, arg_params, aux_params = mx.model.load_checkpoint('caffenet', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fine_tune_model(symbol, arg_params, num_classes, layer_name='drop7'):\n",
    "    \"\"\"\n",
    "    symbol: the pre-trained network symbol\n",
    "    arg_params: the argument parameters of the pre-trained model\n",
    "    num_classes: the number of classes for the fine-tune datasets\n",
    "    layer_name: the layer name before the last fully-connected layer\n",
    "    \"\"\"\n",
    "    all_layers = symbol.get_internals()\n",
    "    net = all_layers[layer_name+'_output']\n",
    "    net = mx.symbol.FullyConnected(data=net, num_hidden=num_classes, name='fc8')\n",
    "    net = mx.sym.Custom(data=net, name = 'softmax', neg_w=0.5, pos_w=1.0,\n",
    "                        op_type = 'weighted_logistic_regression')\n",
    "#     net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n",
    "    freeze = [k for k in arg_params if 'fc8' or 'softmax' not in k]\n",
    "    new_args = dict({k:arg_params[k] for k in arg_params if 'fc8' not in k})\n",
    "    return (net, new_args, freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "def fit(symbol, arg_params, aux_params, train, val, batch_size, num_gpus=1, num_epoch=1, fixed_param_names=[]):\n",
    "    devs = [mx.gpu(i) for i in range(num_gpus)] # replace mx.gpu by mx.cpu for CPU training\n",
    "    mod = mx.mod.Module(symbol=symbol, context=devs, fixed_param_names=fixed_param_names)\n",
    "    mod.bind(data_shapes=train.provide_data, label_shapes=train.provide_label)\n",
    "    mod.init_params(initializer=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=1))\n",
    "    mod.set_params(arg_params, aux_params, allow_missing=True)\n",
    "    \n",
    "    mod.fit(train, val, \n",
    "        num_epoch=num_epoch,\n",
    "        batch_end_callback = mx.callback.log_train_metric(100,True),\n",
    "        epoch_end_callback = mx.callback.do_checkpoint(\"caffenet-mxnet-jump-start-weighted\", 1),\n",
    "        kvstore='device',\n",
    "        optimizer='sgd',\n",
    "        optimizer_params={'learning_rate':0.0009},\n",
    "        eval_metric='acc')\n",
    "#        eval_metric=mx.metric.CrossEntropy())\n",
    "    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-18 12:45:10,900 Already bound, ignoring bind()\n",
      "/home/srip_face/.local/lib/python2.7/site-packages/mxnet/module/base_module.py:464: UserWarning: Parameters already initialized and force_init=False. init_params call ignored.\n",
      "  allow_missing=allow_missing, force_init=force_init)\n",
      "2017-09-18 12:45:21,547 Iter[0] Batch[0] Train-accuracy=0.273438\n",
      "2017-09-18 12:45:43,663 Iter[0] Batch[100] Train-accuracy=0.794766\n",
      "2017-09-18 12:46:05,702 Iter[0] Batch[200] Train-accuracy=0.822812\n",
      "2017-09-18 12:46:11,426 Epoch[0] Train-accuracy=0.824820\n",
      "2017-09-18 12:46:11,431 Epoch[0] Time cost=57.420\n",
      "2017-09-18 12:46:14,336 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0001.params\"\n",
      "2017-09-18 12:46:17,880 Epoch[0] Validation-accuracy=0.790956\n",
      "2017-09-18 12:46:17,984 Iter[1] Batch[0] Train-accuracy=0.804688\n",
      "2017-09-18 12:46:40,018 Iter[1] Batch[100] Train-accuracy=0.832578\n",
      "2017-09-18 12:47:02,039 Iter[1] Batch[200] Train-accuracy=0.842187\n",
      "2017-09-18 12:47:07,758 Epoch[1] Train-accuracy=0.836238\n",
      "2017-09-18 12:47:07,761 Epoch[1] Time cost=49.878\n",
      "2017-09-18 12:47:10,562 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0002.params\"\n",
      "2017-09-18 12:47:13,492 Epoch[1] Validation-accuracy=0.792377\n",
      "2017-09-18 12:47:13,598 Iter[2] Batch[0] Train-accuracy=0.843750\n",
      "2017-09-18 12:47:35,627 Iter[2] Batch[100] Train-accuracy=0.840859\n",
      "2017-09-18 12:47:57,647 Iter[2] Batch[200] Train-accuracy=0.850781\n",
      "2017-09-18 12:48:03,143 Epoch[2] Train-accuracy=0.843750\n",
      "2017-09-18 12:48:03,148 Epoch[2] Time cost=49.654\n",
      "2017-09-18 12:48:07,664 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0003.params\"\n",
      "2017-09-18 12:48:10,493 Epoch[2] Validation-accuracy=0.810791\n",
      "2017-09-18 12:48:10,595 Iter[3] Batch[0] Train-accuracy=0.828125\n",
      "2017-09-18 12:48:32,628 Iter[3] Batch[100] Train-accuracy=0.850000\n",
      "2017-09-18 12:48:54,705 Iter[3] Batch[200] Train-accuracy=0.856250\n",
      "2017-09-18 12:49:00,428 Epoch[3] Train-accuracy=0.841947\n",
      "2017-09-18 12:49:00,432 Epoch[3] Time cost=49.937\n",
      "2017-09-18 12:49:06,187 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0004.params\"\n",
      "2017-09-18 12:49:09,124 Epoch[3] Validation-accuracy=0.811080\n",
      "2017-09-18 12:49:09,231 Iter[4] Batch[0] Train-accuracy=0.851562\n",
      "2017-09-18 12:49:31,268 Iter[4] Batch[100] Train-accuracy=0.857109\n",
      "2017-09-18 12:49:53,305 Iter[4] Batch[200] Train-accuracy=0.861172\n",
      "2017-09-18 12:49:59,032 Epoch[4] Train-accuracy=0.847656\n",
      "2017-09-18 12:49:59,036 Epoch[4] Time cost=49.910\n",
      "2017-09-18 12:50:01,927 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0005.params\"\n",
      "2017-09-18 12:50:04,755 Epoch[4] Validation-accuracy=0.827637\n",
      "2017-09-18 12:50:04,866 Iter[5] Batch[0] Train-accuracy=0.851562\n",
      "2017-09-18 12:50:26,912 Iter[5] Batch[100] Train-accuracy=0.860000\n",
      "2017-09-18 12:50:48,942 Iter[5] Batch[200] Train-accuracy=0.863516\n",
      "2017-09-18 12:50:54,440 Epoch[5] Train-accuracy=0.865000\n",
      "2017-09-18 12:50:54,443 Epoch[5] Time cost=49.685\n",
      "2017-09-18 12:50:57,356 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0006.params\"\n",
      "2017-09-18 12:51:00,277 Epoch[5] Validation-accuracy=0.825047\n",
      "2017-09-18 12:51:00,379 Iter[6] Batch[0] Train-accuracy=0.843750\n",
      "2017-09-18 12:51:22,417 Iter[6] Batch[100] Train-accuracy=0.862734\n",
      "2017-09-18 12:51:44,441 Iter[6] Batch[200] Train-accuracy=0.872969\n",
      "2017-09-18 12:51:50,151 Epoch[6] Train-accuracy=0.865685\n",
      "2017-09-18 12:51:50,154 Epoch[6] Time cost=49.874\n",
      "2017-09-18 12:51:53,045 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0007.params\"\n",
      "2017-09-18 12:51:55,886 Epoch[6] Validation-accuracy=0.822754\n",
      "2017-09-18 12:51:55,989 Iter[7] Batch[0] Train-accuracy=0.867188\n",
      "2017-09-18 12:52:18,015 Iter[7] Batch[100] Train-accuracy=0.866250\n",
      "2017-09-18 12:52:40,030 Iter[7] Batch[200] Train-accuracy=0.875156\n",
      "2017-09-18 12:52:45,750 Epoch[7] Train-accuracy=0.861178\n",
      "2017-09-18 12:52:45,752 Epoch[7] Time cost=49.863\n",
      "2017-09-18 12:52:48,599 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0008.params\"\n",
      "2017-09-18 12:52:51,516 Epoch[7] Validation-accuracy=0.834280\n",
      "2017-09-18 12:52:51,621 Iter[8] Batch[0] Train-accuracy=0.828125\n",
      "2017-09-18 12:53:13,656 Iter[8] Batch[100] Train-accuracy=0.873906\n",
      "2017-09-18 12:53:35,687 Iter[8] Batch[200] Train-accuracy=0.876016\n",
      "2017-09-18 12:53:41,180 Epoch[8] Train-accuracy=0.869687\n",
      "2017-09-18 12:53:41,182 Epoch[8] Time cost=49.664\n",
      "2017-09-18 12:53:44,052 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0009.params\"\n",
      "2017-09-18 12:53:46,977 Epoch[8] Validation-accuracy=0.835938\n",
      "2017-09-18 12:53:47,079 Iter[9] Batch[0] Train-accuracy=0.875000\n",
      "2017-09-18 12:54:09,106 Iter[9] Batch[100] Train-accuracy=0.873906\n",
      "2017-09-18 12:54:31,144 Iter[9] Batch[200] Train-accuracy=0.875859\n",
      "2017-09-18 12:54:36,854 Epoch[9] Train-accuracy=0.875300\n",
      "2017-09-18 12:54:36,855 Epoch[9] Time cost=49.876\n",
      "2017-09-18 12:54:39,728 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0010.params\"\n",
      "2017-09-18 12:54:42,562 Epoch[9] Validation-accuracy=0.836670\n",
      "2017-09-18 12:54:42,669 Iter[10] Batch[0] Train-accuracy=0.921875\n",
      "2017-09-18 12:55:04,688 Iter[10] Batch[100] Train-accuracy=0.879062\n",
      "2017-09-18 12:55:26,708 Iter[10] Batch[200] Train-accuracy=0.879766\n",
      "2017-09-18 12:55:32,424 Epoch[10] Train-accuracy=0.872296\n",
      "2017-09-18 12:55:32,425 Epoch[10] Time cost=49.860\n",
      "2017-09-18 12:55:35,053 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0011.params\"\n",
      "2017-09-18 12:55:37,979 Epoch[10] Validation-accuracy=0.839489\n",
      "2017-09-18 12:55:38,086 Iter[11] Batch[0] Train-accuracy=0.906250\n",
      "2017-09-18 12:56:00,110 Iter[11] Batch[100] Train-accuracy=0.880000\n",
      "2017-09-18 12:56:22,132 Iter[11] Batch[200] Train-accuracy=0.883516\n",
      "2017-09-18 12:56:27,628 Epoch[11] Train-accuracy=0.881250\n",
      "2017-09-18 12:56:27,631 Epoch[11] Time cost=49.649\n",
      "2017-09-18 12:56:30,268 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0012.params\"\n",
      "2017-09-18 12:56:33,099 Epoch[11] Validation-accuracy=0.842285\n",
      "2017-09-18 12:56:33,201 Iter[12] Batch[0] Train-accuracy=0.906250\n",
      "2017-09-18 12:56:55,235 Iter[12] Batch[100] Train-accuracy=0.879766\n",
      "2017-09-18 12:57:17,275 Iter[12] Batch[200] Train-accuracy=0.881172\n",
      "2017-09-18 12:57:22,979 Epoch[12] Train-accuracy=0.888822\n",
      "2017-09-18 12:57:22,981 Epoch[12] Time cost=49.875\n",
      "2017-09-18 12:57:25,619 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0013.params\"\n",
      "2017-09-18 12:57:28,543 Epoch[12] Validation-accuracy=0.832860\n",
      "2017-09-18 12:57:28,645 Iter[13] Batch[0] Train-accuracy=0.898438\n",
      "2017-09-18 12:57:50,681 Iter[13] Batch[100] Train-accuracy=0.884375\n",
      "2017-09-18 12:58:12,709 Iter[13] Batch[200] Train-accuracy=0.883281\n",
      "2017-09-18 12:58:18,431 Epoch[13] Train-accuracy=0.891526\n",
      "2017-09-18 12:58:18,432 Epoch[13] Time cost=49.887\n",
      "2017-09-18 12:58:21,057 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0014.params\"\n",
      "2017-09-18 12:58:23,896 Epoch[13] Validation-accuracy=0.842529\n",
      "2017-09-18 12:58:24,002 Iter[14] Batch[0] Train-accuracy=0.882812\n",
      "2017-09-18 12:58:46,032 Iter[14] Batch[100] Train-accuracy=0.885078\n",
      "2017-09-18 12:59:08,065 Iter[14] Batch[200] Train-accuracy=0.893203\n",
      "2017-09-18 12:59:13,559 Epoch[14] Train-accuracy=0.882188\n",
      "2017-09-18 12:59:13,560 Epoch[14] Time cost=49.663\n",
      "2017-09-18 12:59:16,166 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0015.params\"\n",
      "2017-09-18 12:59:19,088 Epoch[14] Validation-accuracy=0.843750\n",
      "2017-09-18 12:59:19,190 Iter[15] Batch[0] Train-accuracy=0.914062\n",
      "2017-09-18 12:59:41,228 Iter[15] Batch[100] Train-accuracy=0.888516\n",
      "2017-09-18 13:00:03,262 Iter[15] Batch[200] Train-accuracy=0.889609\n",
      "2017-09-18 13:00:08,968 Epoch[15] Train-accuracy=0.881310\n",
      "2017-09-18 13:00:08,970 Epoch[15] Time cost=49.880\n",
      "2017-09-18 13:00:11,614 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0016.params\"\n",
      "2017-09-18 13:00:14,443 Epoch[15] Validation-accuracy=0.855957\n",
      "2017-09-18 13:00:14,544 Iter[16] Batch[0] Train-accuracy=0.867188\n",
      "2017-09-18 13:00:36,573 Iter[16] Batch[100] Train-accuracy=0.892031\n",
      "2017-09-18 13:00:58,600 Iter[16] Batch[200] Train-accuracy=0.890312\n",
      "2017-09-18 13:01:04,318 Epoch[16] Train-accuracy=0.887620\n",
      "2017-09-18 13:01:04,320 Epoch[16] Time cost=49.875\n",
      "2017-09-18 13:01:06,925 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0017.params\"\n",
      "2017-09-18 13:01:10,402 Epoch[16] Validation-accuracy=0.844460\n",
      "2017-09-18 13:01:10,509 Iter[17] Batch[0] Train-accuracy=0.906250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-18 13:01:32,562 Iter[17] Batch[100] Train-accuracy=0.890781\n",
      "2017-09-18 13:01:54,598 Iter[17] Batch[200] Train-accuracy=0.890234\n",
      "2017-09-18 13:02:00,096 Epoch[17] Train-accuracy=0.890312\n",
      "2017-09-18 13:02:00,098 Epoch[17] Time cost=49.693\n",
      "2017-09-18 13:02:02,732 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0018.params\"\n",
      "2017-09-18 13:02:05,655 Epoch[17] Validation-accuracy=0.855824\n",
      "2017-09-18 13:02:05,758 Iter[18] Batch[0] Train-accuracy=0.921875\n",
      "2017-09-18 13:02:27,786 Iter[18] Batch[100] Train-accuracy=0.892578\n",
      "2017-09-18 13:02:49,813 Iter[18] Batch[200] Train-accuracy=0.896016\n",
      "2017-09-18 13:02:55,534 Epoch[18] Train-accuracy=0.893930\n",
      "2017-09-18 13:02:55,536 Epoch[18] Time cost=49.879\n",
      "2017-09-18 13:02:58,173 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0019.params\"\n",
      "2017-09-18 13:03:01,001 Epoch[18] Validation-accuracy=0.847900\n",
      "2017-09-18 13:03:01,102 Iter[19] Batch[0] Train-accuracy=0.914062\n",
      "2017-09-18 13:03:23,124 Iter[19] Batch[100] Train-accuracy=0.895859\n",
      "2017-09-18 13:03:45,152 Iter[19] Batch[200] Train-accuracy=0.896016\n",
      "2017-09-18 13:03:50,873 Epoch[19] Train-accuracy=0.887320\n",
      "2017-09-18 13:03:50,874 Epoch[19] Time cost=49.872\n",
      "2017-09-18 13:03:54,851 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0020.params\"\n",
      "2017-09-18 13:03:57,775 Epoch[19] Validation-accuracy=0.851326\n",
      "2017-09-18 13:03:57,884 Iter[20] Batch[0] Train-accuracy=0.929688\n",
      "2017-09-18 13:04:19,950 Iter[20] Batch[100] Train-accuracy=0.894297\n",
      "2017-09-18 13:04:41,965 Iter[20] Batch[200] Train-accuracy=0.895312\n",
      "2017-09-18 13:04:47,458 Epoch[20] Train-accuracy=0.891875\n",
      "2017-09-18 13:04:47,460 Epoch[20] Time cost=49.681\n",
      "2017-09-18 13:04:50,060 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0021.params\"\n",
      "2017-09-18 13:04:52,888 Epoch[20] Validation-accuracy=0.854980\n",
      "2017-09-18 13:04:52,992 Iter[21] Batch[0] Train-accuracy=0.890625\n",
      "2017-09-18 13:05:15,021 Iter[21] Batch[100] Train-accuracy=0.896797\n",
      "2017-09-18 13:05:37,029 Iter[21] Batch[200] Train-accuracy=0.898750\n",
      "2017-09-18 13:05:42,753 Epoch[21] Train-accuracy=0.901442\n",
      "2017-09-18 13:05:42,756 Epoch[21] Time cost=49.866\n",
      "2017-09-18 13:05:45,389 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0022.params\"\n",
      "2017-09-18 13:05:48,309 Epoch[21] Validation-accuracy=0.847775\n",
      "2017-09-18 13:05:48,409 Iter[22] Batch[0] Train-accuracy=0.875000\n",
      "2017-09-18 13:06:10,443 Iter[22] Batch[100] Train-accuracy=0.899062\n",
      "2017-09-18 13:06:32,473 Iter[22] Batch[200] Train-accuracy=0.899844\n",
      "2017-09-18 13:06:38,188 Epoch[22] Train-accuracy=0.896935\n",
      "2017-09-18 13:06:38,190 Epoch[22] Time cost=49.880\n",
      "2017-09-18 13:06:47,303 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0023.params\"\n",
      "2017-09-18 13:06:50,136 Epoch[22] Validation-accuracy=0.846924\n",
      "2017-09-18 13:06:50,245 Iter[23] Batch[0] Train-accuracy=0.906250\n",
      "2017-09-18 13:07:12,291 Iter[23] Batch[100] Train-accuracy=0.895547\n",
      "2017-09-18 13:07:34,335 Iter[23] Batch[200] Train-accuracy=0.902656\n",
      "2017-09-18 13:07:39,827 Epoch[23] Train-accuracy=0.902813\n",
      "2017-09-18 13:07:39,829 Epoch[23] Time cost=49.690\n",
      "2017-09-18 13:07:44,965 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0024.params\"\n",
      "2017-09-18 13:07:48,011 Epoch[23] Validation-accuracy=0.852746\n",
      "2017-09-18 13:07:48,124 Iter[24] Batch[0] Train-accuracy=0.906250\n",
      "2017-09-18 13:08:10,166 Iter[24] Batch[100] Train-accuracy=0.899375\n",
      "2017-09-18 13:08:32,197 Iter[24] Batch[200] Train-accuracy=0.904531\n",
      "2017-09-18 13:08:37,915 Epoch[24] Train-accuracy=0.895733\n",
      "2017-09-18 13:08:37,917 Epoch[24] Time cost=49.900\n",
      "2017-09-18 13:08:40,559 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0025.params\"\n",
      "2017-09-18 13:08:43,479 Epoch[24] Validation-accuracy=0.857244\n",
      "2017-09-18 13:08:43,584 Iter[25] Batch[0] Train-accuracy=0.890625\n",
      "2017-09-18 13:09:05,607 Iter[25] Batch[100] Train-accuracy=0.903047\n",
      "2017-09-18 13:09:27,623 Iter[25] Batch[200] Train-accuracy=0.903984\n",
      "2017-09-18 13:09:33,343 Epoch[25] Train-accuracy=0.908954\n",
      "2017-09-18 13:09:33,346 Epoch[25] Time cost=49.864\n",
      "2017-09-18 13:09:35,978 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0026.params\"\n",
      "2017-09-18 13:09:38,815 Epoch[25] Validation-accuracy=0.847412\n",
      "2017-09-18 13:09:38,922 Iter[26] Batch[0] Train-accuracy=0.898438\n",
      "2017-09-18 13:10:00,967 Iter[26] Batch[100] Train-accuracy=0.905078\n",
      "2017-09-18 13:10:22,989 Iter[26] Batch[200] Train-accuracy=0.906641\n",
      "2017-09-18 13:10:28,486 Epoch[26] Train-accuracy=0.907500\n",
      "2017-09-18 13:10:28,489 Epoch[26] Time cost=49.672\n",
      "2017-09-18 13:10:31,101 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0027.params\"\n",
      "2017-09-18 13:10:34,023 Epoch[26] Validation-accuracy=0.855587\n",
      "2017-09-18 13:10:34,126 Iter[27] Batch[0] Train-accuracy=0.937500\n",
      "2017-09-18 13:10:56,154 Iter[27] Batch[100] Train-accuracy=0.904609\n",
      "2017-09-18 13:11:18,182 Iter[27] Batch[200] Train-accuracy=0.905859\n",
      "2017-09-18 13:11:23,890 Epoch[27] Train-accuracy=0.900240\n",
      "2017-09-18 13:11:23,892 Epoch[27] Time cost=49.867\n",
      "2017-09-18 13:11:26,539 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0028.params\"\n",
      "2017-09-18 13:11:29,366 Epoch[27] Validation-accuracy=0.870117\n",
      "2017-09-18 13:11:29,469 Iter[28] Batch[0] Train-accuracy=0.890625\n",
      "2017-09-18 13:11:51,492 Iter[28] Batch[100] Train-accuracy=0.904766\n",
      "2017-09-18 13:12:13,516 Iter[28] Batch[200] Train-accuracy=0.907188\n",
      "2017-09-18 13:12:19,232 Epoch[28] Train-accuracy=0.902945\n",
      "2017-09-18 13:12:19,234 Epoch[28] Time cost=49.865\n",
      "2017-09-18 13:12:21,864 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0029.params\"\n",
      "2017-09-18 13:12:24,786 Epoch[28] Validation-accuracy=0.844697\n",
      "2017-09-18 13:12:24,892 Iter[29] Batch[0] Train-accuracy=0.929688\n",
      "2017-09-18 13:12:46,926 Iter[29] Batch[100] Train-accuracy=0.906719\n",
      "2017-09-18 13:13:08,958 Iter[29] Batch[200] Train-accuracy=0.909922\n",
      "2017-09-18 13:13:14,456 Epoch[29] Train-accuracy=0.907500\n",
      "2017-09-18 13:13:14,457 Epoch[29] Time cost=49.668\n",
      "2017-09-18 13:13:17,022 Saved checkpoint to \"caffenet-mxnet-jump-start-weighted-0030.params\"\n",
      "2017-09-18 13:13:19,856 Epoch[29] Validation-accuracy=0.862305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('accuracy', 0.8603219696969697)]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2 # This is binary classification\n",
    "batch_per_gpu = 128\n",
    "num_gpus = 1\n",
    "epoch = 30\n",
    "(new_sym, new_args, freeze) = get_fine_tune_model(sym, arg_params, num_classes)\n",
    "\n",
    "batch_size = batch_per_gpu * num_gpus\n",
    "(train, val) = get_iterators(batch_size)\n",
    "mod = fit(new_sym, new_args, aux_params, train, val, batch_size, num_gpus, epoch)\n",
    "metric = mx.metric.Accuracy()\n",
    "mod_score = mod.score(val, metric)\n",
    "print mod_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prefix = 'caffenet-mxnet-jump-start-128-bootstrap'\n",
    "# mc = mod.save_checkpoint(prefix, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
